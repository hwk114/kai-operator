apiVersion: kai.io/v1alpha1
kind: FineTuneTask
metadata:
  name: llamafactory-full
  namespace: default
spec:
  resources:
    cpu: "8"
    memory: 32Gi
    gpu: "2"
    storage: 100Gi
  volumes:
    - name: model-storage
      type: persistentVolumeClaim
      capacity: 50Gi
      storageClass: standard
      mountPath: /models
    - name: output-storage
      type: persistentVolumeClaim
      capacity: 50Gi
      storageClass: standard
      mountPath: /output
  dataSources:
    - type: pvc
      mountPath: /data
      uri: my-dataset-pvc
  runAfter: []
  ttlSecondsAfterFinished: 3600
  llamafactory:
    image: llamafactory:latest
    version: v0.6.0
    modelPath: /models/llama-7b
    stage: sft
    finetuningType: lora
    trainData: /data/train.jsonl
    validData: /data/valid.jsonl
    outputDir: /output/llama7b-lora
    targetModules:
      - q_proj
      - v_proj
    loraRank: 16
    loraAlpha: 32
    loraDropout: "0.05"
    batchSize: 4
    learningRate: "0.0001"
    numEpochs: 3
    warmupRatio: "0.03"
    maxGradNorm: "1.0"
    saveSteps: 100
    evalSteps: 100
    port: 8000
    routing:
      enabled: true
      gatewayName: default-gateway
      pathPrefix: /llamafactory
    env:
      - name: NCCL_DEBUG
        value: "INFO"
      - name: TRANSFORMERS_CACHE
        value: /output/.cache
    volumes:
      - name: cache
        mountPath: /output/.cache
        readOnly: false
    args:
      - --use_gradient_checkpointing
      - --fp16
